---
title: "Advanced Visualisation"
author: "Nikita Zaitsev"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggpubr)
library(rstatix)
library(GGally)
library(corrplot)
library(RColorBrewer)
library(corrr)
library(factoextra)
library(ggfortify) 
library(plotly)
library(tidymodels)
library(embed)

```

```{r}
original_data <- readRDS('very_low_birthweight.RDS')
```

#1. Сделайте копию датасета, в которой удалите колонки с количеством пропусков больше 100, а затем удалите все строки с пропусками.

```{r}
cleaned_data <- original_data %>%
  mutate(id = row_number()) %>%
  select_if(~ sum(is.na(.)) <= 100) %>%
  drop_na() 

glimpse(cleaned_data)
```

```{r}
cleaned_data <- cleaned_data %>%
  mutate(across(c(twn, vent, pneumo, pda, cld, dead, apg1), ~ as.factor(.))) #Я долго думал, но все-таки решил, что балл опросника будет порядковой переменной, а не количественной, потому что смысла строить для балла опросника непрерывную плотность распределения нет

numeric_data <- cleaned_data %>%
  select(where(is.numeric), -id)

summary(numeric_data)
```

#2. Постройте графики плотности распределения для числовых переменных. Удалите выбросы, если таковые имеются. Преобразуйте категориальные переменные в факторы. Для любых двух числовых переменных раскрасьте график по переменной ‘inout’.

```{r}
numeric_data %>%
  pivot_longer(cols = everything(), values_to = 'value', names_to = 'variable') %>%
  ggplot(aes(x = value)) +
  geom_density(color = 'blue', fill = 'skyblue', alpha = 0.5)+
  labs (title = 'Numeric variables plots')+
  theme_bw()+
  facet_wrap(~variable, scales = 'free')
```


Исходя из саммари количественных данных и их графиков распределения, точно есть выбросы и аномальные значения в переменной hospstay. Возможно, еще может смущать вес ребенка 400гр, но я загуглил, и в Японии в 2019 году был новорожденный весом 268гр, поэтому пусть будет.

Однако, мне вообще ничего не известно о данном датасете, его описание переменных совсем скудно (я так и не понял, что такое hospstay(количество дней в госпитале?)), поэтому исходя из того, что выбросы в переменных это действительно выбросы, а не важные точки, которые следовало бы проанализировать с клиницистом и выявить причину, будем в обучающих целях все-таки убирать эти выбросы из данных по правилу трех сигм.

```{r}
remove_outliers <- function(x, na.rm = TRUE) {
  mean <- mean(x)
  sd <- sd(x)
  upper_bound <- mean + 3 * sd
  lower_bound <- mean - 3 * sd
  ifelse (x >= upper_bound | x <= lower_bound, NA, x)
}

cleaned_data2 <- cleaned_data %>%
  select(-id) %>%
  mutate(across(where(is.numeric), remove_outliers)) %>%
  bind_cols(select(cleaned_data, id)) %>%
  drop_na()
```

Раскрасим получившиеся графики распределений плотности вероятности числовых переменных по категории inout. (В задании указано два графика, но так как я сделал их одним графиком, то и раскрашу одним)

```{r}
cleaned_data2 %>%  
  select(where(is.numeric), inout) %>%
  pivot_longer(!inout, values_to = 'value', names_to = 'variable') %>%
  ggplot(aes(x = value)) +
  geom_density(aes(fill = inout), color = 'blue', alpha = 0.5)+
  theme_bw()+
  facet_wrap(~variable, scales = 'free')
```

#3. Проведите тест на сравнение значений колонки ‘lowph’ между группами в переменной inout. Вид статистического теста определите самостоятельно. Визуализируйте результат через библиотеку 'rstatix'. Как бы вы интерпретировали результат, если бы знали, что более низкое значение lowph ассоциировано с более низкой выживаемостью?

Так как lowph это количественная переменная, то буду использовать параметрический t-тест разницы средних с нулевой гипотезой, что она равна нулю. Выборки независимые, количество наблюдений достаточно велико, поэтому ЦПТ будет работать. Поправки на множественные сравнения делать не буду.

t-тест с поправкой Вэлча (не предполагает равенство дисперсий)

```{r}
t.test(lowph ~ inout, data = cleaned_data2)
```

Так как у нас дисперсии равны, то сделаем парный t-тест без Вэлча (уже с помощью библиотеки rstatix)

```{r}
stat.test <- t_test(lowph ~ inout, data = cleaned_data2, var.equal = TRUE)

stat.test
```

Визуализируем через боксплоты

```{r}
  ggboxplot(cleaned_data2, x = "inout", y = "lowph", fill = "inout")+
  stat_pvalue_manual(stat.test, label = "t-test, p = {p}", y.position = 7.6)
```
В данном тесте у нас была нулевая гипотеза: разница средних значений по выборке lowph между born in Duke и  transported равно нулю. p-значение статистики получилось менее 0.05, что означает, что мы можем отвергнуть нулевую гипотезу. Таким образом, принимая во внимание, что более низкое значение lowph соответствуеь меньшей выживаемости, можем сделать вывод о том, что пациенты в группе transported будут в среднем иметь выживаемость ниже, чем в группе born in Duke, так как у них имеются статистически значимые различия в среднем значении lowph

#4. Сделайте новый датафрейм, в котором оставьте только континуальные или ранговые данные, кроме 'birth', 'year' и 'exit'. Сделайте корреляционный анализ этих данных. Постройте два любых типа графиков для визуализации корреляций.

Первый вариант визуализации корреляции через пакет GGally. В правом верхнем треугольнике графика указаны коэффициенты корреляции между переменными.
```{r message=FALSE}
new_df <- cleaned_data2 %>%
  select(where(is.numeric) & !c(birth, exit, year, id)) #отобрали только континуальные данные. Я не уверен, что hospstay будет континуальным, так как она принимает только целочисленные значения, но пусть будет

ggpairs(new_df)
```

Второй вариант:
```{r}
cor(new_df, method = 'spearman') %>%
  corrplot(method = 'color', type = 'lower', col = COL2('PRGn', 200), diag = FALSE, tl.col = "grey10", addCoef.col = "grey30", cl.pos = "b")
```

Третий вариант: показывает не только корреляционные взаимосвязи, но и отношения близости переменных друг к другу с точки зрения сетевого анализа
```{r}
cor(new_df, method = 'spearman') %>%
  network_plot(min_cor = .0)
```
#5. Постройте иерархическую кластеризацию на этом датафрейме.

```{r warning=FALSE}
df_scaled <- scale(new_df) #шкалируем данные (Z - по шкале)
df_dist <- dist(df_scaled, method = "euclidean") #создаем матрицу дистанций
df_dist.hc <- hclust(d = df_dist,
                     method = "ward.D2") #создаем объект дендрограммы

factoextra::fviz_dend(df_dist.hc, k = 6, rect = TRUE) #визуализируем
  
```
Думаю, это не очень наглядный способ визуализации такого количества наблюдений, так как невозможно глазом идентифицировать какое-то конкретно интересующее наблюдение.

#6. Сделайте одновременный график heatmap и иерархической кластеризации. Интерпретируйте результат.
```{r}
pheatmap(df_scaled, 
         show_rownames = FALSE, 
         clustering_distance_rows = df_dist,
         clustering_method = "ward.D2", 
         cutree_rows = 6,
         cutree_cols = length(colnames(df_scaled)),
         angle_col = 45, 
         main = "Dendrograms for clustering rows and columns with heatmap")
```
Интерпретация: В целом, из данного графика можно посмотреть, как кластеризуются данные (по строкам и столбцам). Можно заметить, что наблюдения с высоким значением hospstay обладают низким значением btw и gest и кластеризуются в один кластер. А относительно низкие значения hospstay соответствуют относительно высоким значениям bwt, gest, lowph, pltct. Также bwt и gest очень похожие между собой колонки значений

#7. Проведите PCA анализ на этих данных. Проинтерпретируйте результат. Нужно ли применять шкалирование для этих данных перед проведением PCA?

Перед проведением PCA необходимо шкалирование данных, так как переменные обладают разными единицами измерения. 

```{r}
new_df.pca <- prcomp(new_df, 
                        scale = T) #можно было взять df_scaled и не писать скалирование внутри функции PCA

summary(new_df.pca)
```
```{r}
fviz_eig(new_df.pca, addlabels = T, ylim = c(0, 40))
```
Первые три главные компоненты описывают почти 83% вариабельности данных.
Сами по себе главные компоненты, какой-либо осмысленности внутри себя не несут, это просто сумма векторов исходных переменных с подобранными коэффициентами (1. чтобы была максимальная дисперсия данных у первой компоненты и далее по убывающей, 2. главные компоненты не коррелируют между собой)

```{r}
fviz_pca_var(new_df.pca, col.var = "contrib")
```
Как и было продемонстировано на графике heatmap и иерархической кластеризации, так и здесь видим, что переменные bwt и gest скоррелированы между собой. А hospstay наоборот обладает отрицательной корреляций с этими переменными, как было показано на предыдущем графике. pltct практически нескоррелирована с hospstay, bwt и gest.

Посмотрим из чего состоят первые три главные компоненты (сумма квадратов коэффициентов перед каждым вектором переменной в главной компоненте должна быть равна 1):
```{r}
fviz_contrib(new_df.pca, choice = "var", axes = 1) # 1
fviz_contrib(new_df.pca, choice = "var", axes = 2) # 2
fviz_contrib(new_df.pca, choice = "var", axes = 3) # 3
```

#8. Постройте biplot график для PCA. Раскрасьте его по значению колонки 'dead'.
```{r}
new_df_with_dead <- cleaned_data2 %>%
  select(where(is.numeric), dead, -c(birth, exit, year))

biplot <- ggbiplot(new_df.pca, 
         scale=0, 
         groups = new_df_with_dead$dead, 
         ellipse = T,
         alpha = 0.5) +
  theme_bw()

biplot
```

#9. Переведите последний график в 'plotly'. При наведении на точку нужно, чтобы отображалось id пациента.
```{r warning=FALSE}
ggplotly(
    biplot+
    geom_point(size = 2,alpha = 0.1, aes(color = new_df_with_dead$dead, text = new_df_with_dead$id)),
  tooltip = "text"
  )
```

#10. Дайте содержательную интерпретацию PCA анализу. Почему использовать колонку 'dead' для выводов об ассоциации с выживаемостью некорректно? 

PCA анализ представляет данные в другом виде, не меняя их качественно (разве что снимает корелляцию) и не делая никаких выводов. Главная компонента считается по формуле: aX1 + bX2 + cX3..... a, b, c это скалярные коэффициенты перед векторами переменных исходного датасета X1, X2, X3. По сути, смыслом метода PCA является поиск оптимальных значений коэффициентов для максимизации дисперсии данных первой главной компоненты. Затем остальных главных компонент при условии, что они не должны быть коллинеарны с другими главными компонентами.

PCA анализ является лишь методом поиска каких-либо "интересностей" (ассоциаций) между переменными, то делать какие-либо выводы об ассоциации пермеменных - неверно. По факту, здесь нет никакого статистического анализа, никаких p-value и т.д. Единственное, что мы делаем это визуализируем данные немного по-другому, что дает нам лишь подсказку, в каком направлении далее можно продолжать проводить исследование данных. Также данный метод дает нам картинку (график выше) лишь 68% вариабельности данных, что опять же не дает нам право делать какие-то статистически значимые выводы.

#11. Приведите ваши данные к размерности в две колонки через UMAP. Сравните результаты отображения точек между алгоритмами PCA и UMAP.
```{r}
umap_prep <- recipe(~., data = new_df) %>% # "техническая" строка, нужная для работы фреймворка tidymodels
  step_normalize(all_predictors()) %>% # нормируем все колонки
  step_umap(all_predictors()) %>%  # проводим в UMAP. Используем стандартные настройки. Чтобы менять ключевой параметр (neighbors), нужно больше погружаться в машинное обучение
  prep() %>%  # "техническая" строка, нужная для работы фреймворка tidymodels. Мы выполняем все степы выше 
  juice() # Финальная строка - приводим результаты UMAP к стандартизированному датасету

umap_prep %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = new_df_with_dead$dead), alpha = 0.7, size = 2) +
  labs(color = NULL) 
```
Как мне кажется, если сравнивать PCA и UMAP, то можно заметить схожесть, что оба метода уменьшают корреляцию в визуализации данных. Однако UMAP больше группирует данные в "сгустки" похожих наблюдений/строк.
Также можно сказать, что умершие пациенты тоже по другому отображены на грфике (на разных точках), что в целом логично, так как оси PCA и UMAP очевидно отличаются.

#12. Давайте самостоятельно увидим, что снижение размерности – это группа методов, славящаяся своей неустойчивостью. Измените основные параметры UMAP (n_neighbors и min_dist) и проанализируйте, как это влияет на результаты.
```{r}
recipe(~., data = new_df) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_predictors(), neighbors = 5) %>%
  prep() %>%  
  juice() %>%
  
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = new_df_with_dead$dead), alpha = 0.7, size = 2) +
  labs(color = NULL) 

#################################

recipe(~., data = new_df) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_predictors(), neighbors = 30) %>% 
  prep() %>%  
  juice() %>%
  
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = new_df_with_dead$dead), alpha = 0.7, size = 2) +
  labs(color = NULL) 
```
При уменьшении числа ближайших соседей (дефолтно 15) сгустки данных становятся более "гуще", так как каждая точка (строка данных) группируется с меньшим числом ближайших соседей. При увеличении же количества ближайших соседей - уже сложно выделить какие-то обособленные сгустки.

```{r}
recipe(~., data = new_df) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_predictors(), min_dist = 0.1) %>% 
  prep() %>%  
  juice() %>%
  
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = new_df_with_dead$dead), alpha = 0.7, size = 2) +
  labs(color = NULL) 

############################
recipe(~., data = new_df) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_predictors(), min_dist = 0.5) %>% 
  prep() %>%  
  juice() %>%
  
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = new_df_with_dead$dead), alpha = 0.7, size = 2) +
  labs(color = NULL) 

```
Параметр min_dist также сильно влияет на расположение точек на графике. Поэтому такой метод визуализации данных неустояив к заданным параметрам отображения. Необходимо точно подбирать параметры для достижения необходимой цели визуализации.
